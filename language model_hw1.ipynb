{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9afbea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb14902f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\15531\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\15531\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52a24efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the IMDB dataset (adjust the path as necessary)\n",
    "df = pd.read_csv('IMDB Dataset.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8563d8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'review' contains the text and 'sentiment' labels (positive/negative)\n",
    "positive_tokens = []\n",
    "negative_tokens = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87968754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set of stopwords to filter out\n",
    "unwanted_tokens = set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "712dddcd",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----start----\n"
     ]
    }
   ],
   "source": [
    "print(\"----start----\")\n",
    "# Iterate over the dataset and process each review\n",
    "for _, row in df.iterrows():\n",
    "    review = row['review']\n",
    "    sentiment = row['sentiment']\n",
    "    \n",
    "    # Normalize review: lowercasing and removing non-word characters\n",
    "    cleaned_review = re.sub(r'<.*?>', ' ', review.lower())\n",
    "     \n",
    "     # Tokenize the cleaned review\n",
    "    tokens = word_tokenize(cleaned_review)\n",
    "     \n",
    "     # Filter out unwanted tokens\n",
    "    filtered_tokens = [token for token in tokens if not token.isnumeric() and token not in unwanted_tokens and token not in [\"''\", '``','*','',',','.','br','<','>','!','(',')','--','?'] ]\n",
    "     \n",
    "     # Add tokens to respective lists based on sentiment\n",
    "    if sentiment == 'positive':\n",
    "        positive_tokens.extend(filtered_tokens)\n",
    "    elif sentiment == 'negative':\n",
    "        negative_tokens.extend(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91a6f777",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "from nltk import FreqDist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "007e845f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_size = 6210698\n",
      "vocab_size = 151068\n"
     ]
    }
   ],
   "source": [
    "# get token_size and vocab_size\n",
    "token_size = len(positive_tokens) + len(negative_tokens)\n",
    "print(f\"token_size = {token_size}\")\n",
    "token_all = positive_tokens + negative_tokens\n",
    "token_freq = FreqDist(token_all)\n",
    "vocab_size = len(token_freq)\n",
    "print(f\"vocab_size = {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a15ecae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigram_freq_pos.most_common(10):[(('ca', \"n't\"), 2858), (('film', \"'s\"), 1681), (('one', 'best'), 1654), ((\"'ve\", 'seen'), 1332), ((\"n't\", 'know'), 1238), (('wo', \"n't\"), 1215), (('even', 'though'), 1092), (('ever', 'seen'), 959), (('movie', \"'s\"), 945), (('could', \"n't\"), 942)]\n"
     ]
    }
   ],
   "source": [
    "# Assuming positive_tokens is a list of tokenized positive words\n",
    "bigrams_pos = ngrams(positive_tokens, 2)\n",
    "bigram_freq_pos = FreqDist(bigrams_pos)\n",
    "print(f\"bigram_freq_pos.most_common(10):{bigram_freq_pos.most_common(10)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef610e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigram_freq_neg.most_common(10):[(('ca', \"n't\"), 4170), ((\"n't\", 'even'), 2231), (('could', \"n't\"), 2092), (('ever', 'seen'), 1722), ((\"n't\", 'know'), 1674), (('film', \"'s\"), 1438), ((\"'ve\", 'seen'), 1434), (('waste', 'time'), 1420), (('special', 'effects'), 1402), (('would', \"n't\"), 1341)]\n"
     ]
    }
   ],
   "source": [
    "# Assuming negative_tokens is a list of tokenized negative words\n",
    "bigrams_neg = ngrams(negative_tokens, 2)\n",
    "bigram_freq_neg = FreqDist(bigrams_neg)\n",
    "print(f\"bigram_freq_neg.most_common(10):{bigram_freq_neg.most_common(10)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53e8cda4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trigram_freq_pos.most_common(10):[((\"'ve\", 'ever', 'seen'), 388), (('ca', \"n't\", 'help'), 222), (('new', 'york', 'city'), 193), (('ca', \"n't\", 'wait'), 172), (('world', 'war', 'ii'), 158), (('one', 'best', 'movies'), 143), (('based', 'true', 'story'), 133), (('ca', \"n't\", 'get'), 131), (('one', 'best', 'films'), 129), (('ca', \"n't\", 'say'), 126)]\n"
     ]
    }
   ],
   "source": [
    "# Assuming positive_tokens is a list of tokenized positive words\n",
    "trigrams_pos = ngrams(positive_tokens, 3)\n",
    "trigram_freq_pos = FreqDist(trigrams_pos)\n",
    "print(f\"trigram_freq_pos.most_common(10):{trigram_freq_pos.most_common(10)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2ade136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trigram_freq_neg.most_common(10):[((\"'ve\", 'ever', 'seen'), 707), ((\"n't\", 'waste', 'time'), 387), (('ca', \"n't\", 'believe'), 368), (('worst', 'movie', 'ever'), 358), (('one', 'worst', 'movies'), 309), (('ca', \"n't\", 'even'), 242), (('movie', 'ever', 'seen'), 241), (('worst', 'movies', 'ever'), 204), ((\"n't\", 'make', 'sense'), 199), ((\"n't\", 'get', 'wrong'), 188)]\n"
     ]
    }
   ],
   "source": [
    "# Assuming negative_tokens is a list of tokenized negative words\n",
    "\n",
    "trigrams_neg = ngrams(negative_tokens, 3)\n",
    "trigram_freq_neg = FreqDist(trigrams_neg)\n",
    "print(f\"trigram_freq_neg.most_common(10):{trigram_freq_neg.most_common(10)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4da453fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams, FreqDist\n",
    "import math\n",
    "def get_freq_prob(word1, word2, word3):\n",
    "    bigram = list(ngrams(token_all, 2))\n",
    "    trigram = list(ngrams(token_all, 3))\n",
    "    word1_word2_count = FreqDist(bigram)[word1, word2]\n",
    "    word1_word2_word3_count = FreqDist(trigram)[word1,word2,word3]\n",
    "    res = (1 + word1_word2_word3_count)/(vocab_size + word1_word2_count)\n",
    "    print(f'{word3}| {word1}, {word2} = {word1_word2_count}, {word1_word2_word3_count}, {res}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3a7199e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "story|based,true=213,173,0.0011501774842842128\n",
      "city|new,york=1299,273,0.001798289655896618\n",
      "ever|worst,movie=728,365,0.0024111307280824264\n",
      "movies|one,worst=983,313,0.0020650965794371625\n",
      "am|hi,i=0,0,6.619535573384171e-06\n"
     ]
    }
   ],
   "source": [
    "# Print the trigram probabilities\n",
    "get_freq_prob('based', 'true', 'story')\n",
    "get_freq_prob('new', 'york', 'city')\n",
    "get_freq_prob('worst', 'movie', 'ever')\n",
    "get_freq_prob('one', 'worst', 'movies')\n",
    "get_freq_prob('hi','i', 'am')\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3.12.5 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "921f6f0f1f38bf7566fbae797308a1378c76d769e338788073a679acaba81427"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
